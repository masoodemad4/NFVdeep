env is <Env instance>
Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
properties are: {'num_nodes': 12, 'num_node_resources': 3}
Logging to /workspaces/NFVdeep/resuls/logs/PPO_26
-----------------------------
| time/              |      |
|    fps             | 724  |
|    iterations      | 1    |
|    time_elapsed    | 2    |
|    total_timesteps | 2048 |
-----------------------------
properties are: {'num_nodes': 12, 'num_node_resources': 3}
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 2.44e+03     |
|    ep_rew_mean          | 3.97e+05     |
| time/                   |              |
|    fps                  | 628          |
|    iterations           | 2            |
|    time_elapsed         | 6            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0016919855 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.56        |
|    explained_variance   | 2.62e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 9.54e+06     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00397     |
|    value_loss           | 2.14e+07     |
------------------------------------------
properties are: {'num_nodes': 12, 'num_node_resources': 3}
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.49e+03      |
|    ep_rew_mean          | 4.15e+05      |
| time/                   |               |
|    fps                  | 600           |
|    iterations           | 3             |
|    time_elapsed         | 10            |
|    total_timesteps      | 6144          |
| train/                  |               |
|    approx_kl            | 0.00065138063 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.56         |
|    explained_variance   | 5.36e-07      |
|    learning_rate        | 0.0003        |
|    loss                 | 8.17e+06      |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.00162      |
|    value_loss           | 1.88e+07      |
-------------------------------------------
properties are: {'num_nodes': 12, 'num_node_resources': 3}
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 2.5e+03       |
|    ep_rew_mean          | 4.4e+05       |
| time/                   |               |
|    fps                  | 590           |
|    iterations           | 4             |
|    time_elapsed         | 13            |
|    total_timesteps      | 8192          |
| train/                  |               |
|    approx_kl            | 0.00078270875 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.56         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 1.12e+07      |
|    n_updates            | 30            |
|    policy_gradient_loss | -0.00232      |
|    value_loss           | 2.24e+07      |
-------------------------------------------
properties are: {'num_nodes': 12, 'num_node_resources': 3}
properties are: {'num_nodes': 12, 'num_node_resources': 3}
properties are: {'num_nodes': 12, 'num_node_resources': 3}
properties are: {'num_nodes': 12, 'num_node_resources': 3}
properties are: {'num_nodes': 12, 'num_node_resources': 3}
properties are: {'num_nodes': 12, 'num_node_resources': 3}
Eval num_timesteps=10000, episode_reward=475813.98 +/- 28745.98
Episode length: 2589.40 +/- 35.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 2.59e+03      |
|    mean_reward          | 4.76e+05      |
| time/                   |               |
|    total_timesteps      | 10000         |
| train/                  |               |
|    approx_kl            | 0.00052511366 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.56         |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0003        |
|    loss                 | 8.86e+06      |
|    n_updates            | 40            |
|    policy_gradient_loss | -0.00148      |
|    value_loss           | 2.5e+07       |
-------------------------------------------
New best mean reward!
properties are: {'num_nodes': 12, 'num_node_resources': 3}
--------------------------------------------
| eval/                         |          |
|    acceptance_ratio           | 0.485    |
|    mean_bandwidth_cost        | 3.17     |
|    mean_bandwidth_utilization | 1.78e+03 |
|    mean_cpu_cost              | 468      |
|    mean_cpu_utilization       | 662      |
|    mean_memory_cost           | 68.4     |
|    mean_memory_utilization    | 3.59e+03 |
|    mean_operating_servers     | 8.63     |
|    rejection_ratio            | 0.515    |
| rollout/                      |          |
|    ep_len_mean                | 2.5e+03  |
|    ep_rew_mean                | 4.54e+05 |
| time/                         |          |
|    fps                        | 292      |
|    iterations                 | 5        |
|    time_elapsed               | 34       |
|    total_timesteps            | 10240    |
--------------------------------------------
